# [[EEML2024](https://www.eeml.eu)] Tutorial 3: Vision-Language Models

**Authors:** Aishwarya Kamath, Anastasia IliÄ‡ and Ioana Bica

--- 
In this tutorial we'll explore how we can use image-text data to build Vision Language Models ðŸš€. We'll start with an introduction to multimodal understanding that describes the main components of a Vision Lanugage Model and provides a brief history of how these have evolved in recent years. Then, we'll dive deep into Contrastive Language-Image Pre-training (CLIP), a model for learning general representation from image-text pairs that can be used for a wide range of downstream tasks. We'll then explore how CLIP can be used for semantic image search followed by a showcase of its failure cases. Finally, we'll finetune together PaliGemma, a powerful 3B vision language model.

### Outline
- Part I: Introduction to multimodal understanding.
- Part II: Contrastive Language-Image Pre-training (CLIP).
- Part III: PaliGemma.

### Notebooks

Tutorial: [![Open In 
Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eemlcommunity/PracticalSessions2024/blob/main/3_vision_language_models/VLM_tutorial.ipynb)

---
